---
title: "hw7"
author: "zzf"
date: "2018年12月27日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR)
#探索数据
class(OJ)
head(OJ)
summary(OJ)
plot(OJ$Purchase,OJ$Store7)
#该图说明，MM产品在第七分店的销量比CH产品更差；大部分MM与CH产品是在别的分店卖的，不是第七分店

#将数据分为训练部分和测试部分，前者有800个观测点
trainindex<-sample(1:nrow(OJ),800,replace = F)
traindata<-OJ[trainindex,]
library(tree)
tree.oj<-tree(Purchase~., data = traindata)
summary(tree.oj)
#所以，由训练数据产生的树有7个节点，训练错误率为16.75%

#使用测试数据
set.seed(10)
tree.pred<-predict(tree.oj, OJ[-trainindex,],type ="class")
summary(tree.pred)
table(tree.pred,OJ[-trainindex,"Purchase"])#第一个代表行，第二个代表列

1-mean(tree.pred!=OJ[-trainindex,"Purchase"])
#即预测准确率大约是81.85%

#使用cv.tree修剪tree
cv.oj<-cv.tree(tree.oj,FUN = prune.misclass)
cv.oj #size表示节点的个数，dev在FUN等于prune.misclass的情况下是分类错误率，k表示惩罚参数α
par(mfrow = c(1, 2), mar = c(5, 5, 1, 0.1))
plot(cv.oj$size,cv.oj$dev,type = "b", xlab = "size", ylab = "Error rate", col = "blue", lwd = 3)
plot(cv.oj$k,cv.oj$dev,type = "b", xlab = "size", ylab = "Error rate", col = "red", lwd = 3)

#由图可得，当size（节点个数）为2，α为0的时候，训练错误率最小
#修剪
prune.oj<-prune.misclass(tree.oj,best = 4)
plot(prune.oj)
text(prune.oj,pretty = 0)
#计算修剪后的预测准确率
pred.oj<-predict(prune.oj,OJ[-trainindex,],type = "class")
table(pred.oj,OJ[-trainindex,"Purchase"])
1-mean(pred.oj!=OJ[-trainindex,"Purchase"])
#修剪后的新树下的预测准确率是79.629%，比前面略低一些。

#用bagging模型
library(randomForest)
set.seed(50)
ncol(OJ)
oj.bag<-randomForest(Purchase~.,data = OJ[trainindex,],importance = T, mtry = 17)
oj.bag
#计算预测准确率
pred.bag.oj<-predict(oj.bag,OJ[-trainindex,],type = "class")
1-mean(pred.bag.oj!=OJ[-trainindex,"Purchase"])
#预测准确率约为78.89%

#找出重要的自变量
importance(oj.bag)
varImpPlot(oj.bag)
#可以由图得到，在bagging方法中，客户忠诚度LoyalCH和价格的差别PriceDiff，对这个回归决策的影响最大

#用randomForest模型
oj.randomforest<-randomForest(Purchase~.,data = OJ[trainindex,],importance = T)
oj.randomforest
pred.oj.rf<-predict(oj.randomforest,OJ[-trainindex,],type = "class")
#计算预测准确率
1-mean(pred.oj.rf!=OJ[-trainindex,"Purchase"])
#在randomforest模型下，模型每次只随机选4个自变量进行分裂，最后得到的模型预测准确率约为80%。

#找出重要的自变量
importance(oj.randomforest)
varImpPlot(oj.randomforest)
#忠诚度LoyalCH显然还是变量中最重要的一个

#使用boosting来建立模型,3000颗树，深度为4
library(gbm)
set.seed(530)
fa<-ifelse(OJ[ ,"Purchase"] == "CH",0,1)

newOJ<-cbind(fa,OJ)

OJ.boost<-gbm(fa~.-Purchase,data = newOJ[trainindex,],distribution = "bernoulli",n.trees = 5000,interaction.depth = 4)
par(mar = c(5, 5, 1, 0.1))
summary(OJ.boost) #boosting模型下认为，LoyalCH这个变量最重要。
plot(OJ.boost, i = "LoyalCH", col ="blue", lwd = 2)
#计算预测准确率

#pred.boost<-predict(OJ.boost,newdata=newOJ[trainindex,],n.trees = 5000,type = "class")




```

